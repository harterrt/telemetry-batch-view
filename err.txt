[0m[[0minfo[0m] [0mLoading project definition from /home/harterrt/Documents/xsec/harter/project[0m
[0m[[0minfo[0m] [0mSet current project to telemetry-batch-view (in build file:/home/harterrt/Documents/xsec/harter/)[0m
[0m[[0minfo[0m] [0mCompiling 1 Scala source to /home/harterrt/Documents/xsec/harter/target/scala-2.10/classes...[0m
[0m[[0minfo[0m] [0mRunning com.mozilla.telemetry.views.CrossSectionalView --localTable=/home/harterrt/data/l10l_20160725_single_shard.parquet --outName=arch_test[0m
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/09/07 11:24:51 INFO SparkContext: Running Spark version 1.6.1
16/09/07 11:24:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/09/07 11:24:51 WARN Utils: Your hostname, harter-laptop resolves to a loopback address: 127.0.1.1; using 192.168.1.154 instead (on interface wlp4s0)
16/09/07 11:24:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/09/07 11:24:51 INFO SecurityManager: Changing view acls to: harterrt
16/09/07 11:24:51 INFO SecurityManager: Changing modify acls to: harterrt
16/09/07 11:24:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(harterrt); users with modify permissions: Set(harterrt)
16/09/07 11:24:52 INFO Utils: Successfully started service 'sparkDriver' on port 43213.
16/09/07 11:24:52 INFO Slf4jLogger: Slf4jLogger started
16/09/07 11:24:52 INFO Remoting: Starting remoting
16/09/07 11:24:52 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.1.154:44124]
16/09/07 11:24:52 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 44124.
16/09/07 11:24:52 INFO SparkEnv: Registering MapOutputTracker
16/09/07 11:24:52 INFO SparkEnv: Registering BlockManagerMaster
16/09/07 11:24:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cdb7d190-a01b-4ae2-9d84-785d7be24ff1
16/09/07 11:24:52 INFO MemoryStore: MemoryStore started with capacity 1140.4 MB
16/09/07 11:24:52 INFO SparkEnv: Registering OutputCommitCoordinator
16/09/07 11:24:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/09/07 11:24:53 INFO SparkUI: Started SparkUI at http://192.168.1.154:4040
16/09/07 11:24:53 INFO Executor: Starting executor ID driver on host localhost
16/09/07 11:24:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37479.
16/09/07 11:24:53 INFO NettyBlockTransferService: Server created on 37479
16/09/07 11:24:53 INFO BlockManagerMaster: Trying to register BlockManager
16/09/07 11:24:53 INFO BlockManagerMasterEndpoint: Registering block manager localhost:37479 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 37479)
16/09/07 11:24:53 INFO BlockManagerMaster: Registered BlockManager
16/09/07 11:24:53 INFO HiveContext: Initializing execution hive, version 1.2.1
16/09/07 11:24:53 INFO ClientWrapper: Inspected Hadoop version: 2.7.1
16/09/07 11:24:53 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.7.1
16/09/07 11:24:53 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/09/07 11:24:54 INFO ObjectStore: ObjectStore, initialize called
16/09/07 11:24:54 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
16/09/07 11:24:54 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/09/07 11:24:57 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16/09/07 11:24:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:24:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:24:59 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:24:59 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:25:00 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
16/09/07 11:25:00 INFO ObjectStore: Initialized ObjectStore
16/09/07 11:25:00 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
16/09/07 11:25:00 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
16/09/07 11:25:00 INFO HiveMetaStore: Added admin role in metastore
16/09/07 11:25:00 INFO HiveMetaStore: Added public role in metastore
16/09/07 11:25:01 INFO HiveMetaStore: No user is added in admin role, since config is empty
16/09/07 11:25:01 INFO HiveMetaStore: 0: get_all_databases
16/09/07 11:25:01 INFO audit: ugi=harterrt	ip=unknown-ip-addr	cmd=get_all_databases	
16/09/07 11:25:01 INFO HiveMetaStore: 0: get_functions: db=default pat=*
16/09/07 11:25:01 INFO audit: ugi=harterrt	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16/09/07 11:25:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:25:01 INFO SessionState: Created local directory: /tmp/9d594ed7-82da-419f-b246-b34092cb20a5_resources
16/09/07 11:25:01 INFO SessionState: Created HDFS directory: /tmp/hive/harterrt/9d594ed7-82da-419f-b246-b34092cb20a5
16/09/07 11:25:01 INFO SessionState: Created local directory: /tmp/harterrt/9d594ed7-82da-419f-b246-b34092cb20a5
16/09/07 11:25:01 INFO SessionState: Created HDFS directory: /tmp/hive/harterrt/9d594ed7-82da-419f-b246-b34092cb20a5/_tmp_space.db
16/09/07 11:25:01 INFO HiveContext: default warehouse location is /user/hive/warehouse
16/09/07 11:25:01 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16/09/07 11:25:01 INFO ClientWrapper: Inspected Hadoop version: 2.7.1
16/09/07 11:25:01 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.7.1
16/09/07 11:25:02 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/09/07 11:25:02 INFO ObjectStore: ObjectStore, initialize called
16/09/07 11:25:02 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
16/09/07 11:25:02 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/09/07 11:25:03 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16/09/07 11:25:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:25:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:25:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:25:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:25:04 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
16/09/07 11:25:04 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
16/09/07 11:25:04 INFO ObjectStore: Initialized ObjectStore
16/09/07 11:25:04 INFO HiveMetaStore: Added admin role in metastore
16/09/07 11:25:04 INFO HiveMetaStore: Added public role in metastore
16/09/07 11:25:04 INFO HiveMetaStore: No user is added in admin role, since config is empty
16/09/07 11:25:05 INFO HiveMetaStore: 0: get_all_databases
16/09/07 11:25:05 INFO audit: ugi=harterrt	ip=unknown-ip-addr	cmd=get_all_databases	
16/09/07 11:25:05 INFO HiveMetaStore: 0: get_functions: db=default pat=*
16/09/07 11:25:05 INFO audit: ugi=harterrt	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16/09/07 11:25:05 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
16/09/07 11:25:05 INFO SessionState: Created local directory: /tmp/fa1a38df-d830-4e74-9b89-4d262476cd2b_resources
16/09/07 11:25:05 INFO SessionState: Created HDFS directory: /tmp/hive/harterrt/fa1a38df-d830-4e74-9b89-4d262476cd2b
16/09/07 11:25:05 INFO SessionState: Created local directory: /tmp/harterrt/fa1a38df-d830-4e74-9b89-4d262476cd2b
16/09/07 11:25:05 INFO SessionState: Created HDFS directory: /tmp/hive/harterrt/fa1a38df-d830-4e74-9b89-4d262476cd2b/_tmp_space.db
HEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHEREHERE
16/09/07 11:25:05 INFO ParquetRelation: Listing file:/home/harterrt/data/l10l_20160725_single_shard.parquet on driver
16/09/07 11:25:05 INFO SparkContext: Starting job: parquet at CrossSectionalView.scala:154
16/09/07 11:25:05 INFO DAGScheduler: Got job 0 (parquet at CrossSectionalView.scala:154) with 4 output partitions
16/09/07 11:25:05 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at CrossSectionalView.scala:154)
16/09/07 11:25:05 INFO DAGScheduler: Parents of final stage: List()
16/09/07 11:25:05 INFO DAGScheduler: Missing parents: List()
16/09/07 11:25:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at CrossSectionalView.scala:154), which has no missing parents
16/09/07 11:25:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 68.0 KB, free 68.0 KB)
16/09/07 11:25:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 91.1 KB)
16/09/07 11:25:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:37479 (size: 23.1 KB, free: 1140.4 MB)
16/09/07 11:25:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/09/07 11:25:05 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at CrossSectionalView.scala:154)
16/09/07 11:25:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/09/07 11:25:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2076 bytes)
16/09/07 11:25:05 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 2076 bytes)
16/09/07 11:25:05 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2,PROCESS_LOCAL, 2076 bytes)
16/09/07 11:25:05 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3,PROCESS_LOCAL, 2201 bytes)
16/09/07 11:25:05 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/09/07 11:25:05 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/09/07 11:25:05 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/09/07 11:25:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/09/07 11:25:05 INFO ParquetFileReader: Initiating action with parallelism: 5
16/09/07 11:25:05 INFO ParquetFileReader: Initiating action with parallelism: 5
16/09/07 11:25:05 INFO ParquetFileReader: Initiating action with parallelism: 5
16/09/07 11:25:05 INFO ParquetFileReader: Initiating action with parallelism: 5
16/09/07 11:25:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 936 bytes result sent to driver
16/09/07 11:25:05 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 936 bytes result sent to driver
16/09/07 11:25:05 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 936 bytes result sent to driver
16/09/07 11:25:05 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 131 ms on localhost (1/4)
16/09/07 11:25:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 160 ms on localhost (2/4)
16/09/07 11:25:05 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 142 ms on localhost (3/4)
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
16/09/07 11:25:06 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 187247 bytes result sent to driver
16/09/07 11:25:06 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 819 ms on localhost (4/4)
16/09/07 11:25:06 INFO DAGScheduler: ResultStage 0 (parquet at CrossSectionalView.scala:154) finished in 0.852 s
16/09/07 11:25:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/09/07 11:25:06 INFO DAGScheduler: Job 0 finished: parquet at CrossSectionalView.scala:154, took 0.982913 s
16/09/07 11:25:06 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/09/07 11:25:06 INFO audit: ugi=harterrt	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
[Ljava.lang.String;@18cda83d
16/09/07 11:25:06 INFO ParseDriver: Parsing command: SELECT * FROM longitudinal
16/09/07 11:25:07 INFO ParseDriver: Parse Completed
16/09/07 11:25:08 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:37479 in memory (size: 23.1 KB, free: 1140.4 MB)
16/09/07 11:25:08 INFO ContextCleaner: Cleaned accumulator 1
16/09/07 11:25:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 96.4 KB, free 96.4 KB)
16/09/07 11:25:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 21.7 KB, free 118.0 KB)
16/09/07 11:25:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:37479 (size: 21.7 KB, free: 1140.4 MB)
16/09/07 11:25:09 INFO SparkContext: Created broadcast 1 from take at CrossSectionalView.scala:169
16/09/07 11:25:10 INFO SparkContext: Starting job: take at CrossSectionalView.scala:169
16/09/07 11:25:10 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
16/09/07 11:25:10 INFO ParquetRelation: Reading Parquet file(s) from file:/home/harterrt/data/l10l_20160725_single_shard.parquet
16/09/07 11:25:10 INFO DAGScheduler: Registering RDD 5 (take at CrossSectionalView.scala:169)
16/09/07 11:25:10 INFO DAGScheduler: Got job 1 (take at CrossSectionalView.scala:169) with 1 output partitions
16/09/07 11:25:10 INFO DAGScheduler: Final stage: ResultStage 2 (take at CrossSectionalView.scala:169)
16/09/07 11:25:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
16/09/07 11:25:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
16/09/07 11:25:10 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at take at CrossSectionalView.scala:169), which has no missing parents
16/09/07 11:25:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 228.0 KB, free 346.0 KB)
16/09/07 11:25:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 69.7 KB, free 415.7 KB)
16/09/07 11:25:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:37479 (size: 69.7 KB, free: 1140.3 MB)
16/09/07 11:25:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
16/09/07 11:25:10 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at take at CrossSectionalView.scala:169)
16/09/07 11:25:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 5 tasks
16/09/07 11:25:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2189 bytes)
16/09/07 11:25:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, localhost, partition 1,PROCESS_LOCAL, 2194 bytes)
16/09/07 11:25:10 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, localhost, partition 2,PROCESS_LOCAL, 2195 bytes)
16/09/07 11:25:10 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, localhost, partition 3,PROCESS_LOCAL, 2195 bytes)
16/09/07 11:25:10 INFO Executor: Running task 1.0 in stage 1.0 (TID 5)
16/09/07 11:25:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)
16/09/07 11:25:10 INFO Executor: Running task 3.0 in stage 1.0 (TID 7)
16/09/07 11:25:10 INFO Executor: Running task 2.0 in stage 1.0 (TID 6)
16/09/07 11:25:10 INFO ParquetRelation$$anonfun$buildInternalScan$1$$anon$1: Input split: ParquetInputSplit{part: file:/home/harterrt/data/l10l_20160725_single_shard.parquet start: 402653184 end: 536870912 length: 134217728 hosts: []}
16/09/07 11:25:10 INFO ParquetRelation$$anonfun$buildInternalScan$1$$anon$1: Input split: ParquetInputSplit{part: file:/home/harterrt/data/l10l_20160725_single_shard.parquet start: 0 end: 134217728 length: 134217728 hosts: []}
16/09/07 11:25:10 INFO ParquetRelation$$anonfun$buildInternalScan$1$$anon$1: Input split: ParquetInputSplit{part: file:/home/harterrt/data/l10l_20160725_single_shard.parquet start: 268435456 end: 402653184 length: 134217728 hosts: []}
16/09/07 11:25:10 INFO ParquetRelation$$anonfun$buildInternalScan$1$$anon$1: Input split: ParquetInputSplit{part: file:/home/harterrt/data/l10l_20160725_single_shard.parquet start: 134217728 end: 268435456 length: 134217728 hosts: []}
16/09/07 11:25:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16/09/07 11:25:11 INFO CatalystReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required binary client_id (UTF8);
  required binary normalized_channel (UTF8);
  optional group geo_country (LIST) {
    repeated binary array (UTF8);
  }
  optional group session_length (LIST) {
    repeated int64 array;
  }
  optional group build (LIST) {
    repeated group array {
      optional binary application_id (UTF8);
      optional binary application_name (UTF8);
      optional binary architecture (UTF8);
      optional binary architectures_in_binary (UTF8);
      optional binary build_id (UTF8);
      optional binary version (UTF8);
      optional binary vendor (UTF8);
      optional binary platform_version (UTF8);
      optional binary xpcom_abi (UTF8);
      optional binary hotfix_version (UTF8);
    }
  }
  optional group settings (LIST) {
    repeated group array {
      optional boolean addon_compatibility_check_enabled;
      optional boolean blocklist_enabled;
      optional boolean is_default_browser;
      optional binary default_search_engine (UTF8);
      optional group default_search_engine_data {
        optional binary name (UTF8);
        optional binary load_path (UTF8);
        optional binary submission_url (UTF8);
      }
      optional binary search_cohort (UTF8);
      optional boolean e10s_enabled;
      optional boolean telemetry_enabled;
      optional binary locale (UTF8);
      optional group update {
        optional binary channel (UTF8);
        optional boolean enabled;
        optional boolean auto_download;
      }
      optional group user_prefs (MAP) {
        repeated group map (MAP_KEY_VALUE) {
          required binary key (UTF8);
          required binary value (UTF8);
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(client_id,StringType,false), StructField(normalized_channel,StringType,false), StructField(geo_country,ArrayType(StringType,false),true), StructField(session_length,ArrayType(LongType,false),true), StructField(build,ArrayType(StructType(StructField(application_id,StringType,true), StructField(application_name,StringType,true), StructField(architecture,StringType,true), StructField(architectures_in_binary,StringType,true), StructField(build_id,StringType,true), StructField(version,StringType,true), StructField(vendor,StringType,true), StructField(platform_version,StringType,true), StructField(xpcom_abi,StringType,true), StructField(hotfix_version,StringType,true)),false),true), StructField(settings,ArrayType(StructType(StructField(addon_compatibility_check_enabled,BooleanType,true), StructField(blocklist_enabled,BooleanType,true), StructField(is_default_browser,BooleanType,true), StructField(default_search_engine,StringType,true), StructField(default_search_engine_data,StructType(StructField(name,StringType,true), StructField(load_path,StringType,true), StructField(submission_url,StringType,true)),true), StructField(search_cohort,StringType,true), StructField(e10s_enabled,BooleanType,true), StructField(telemetry_enabled,BooleanType,true), StructField(locale,StringType,true), StructField(update,StructType(StructField(channel,StringType,true), StructField(enabled,BooleanType,true), StructField(auto_download,BooleanType,true)),true), StructField(user_prefs,MapType(StringType,StringType,false),true)),false),true))
       
16/09/07 11:25:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16/09/07 11:25:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16/09/07 11:25:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16/09/07 11:25:11 INFO CatalystReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required binary client_id (UTF8);
  required binary normalized_channel (UTF8);
  optional group geo_country (LIST) {
    repeated binary array (UTF8);
  }
  optional group session_length (LIST) {
    repeated int64 array;
  }
  optional group build (LIST) {
    repeated group array {
      optional binary application_id (UTF8);
      optional binary application_name (UTF8);
      optional binary architecture (UTF8);
      optional binary architectures_in_binary (UTF8);
      optional binary build_id (UTF8);
      optional binary version (UTF8);
      optional binary vendor (UTF8);
      optional binary platform_version (UTF8);
      optional binary xpcom_abi (UTF8);
      optional binary hotfix_version (UTF8);
    }
  }
  optional group settings (LIST) {
    repeated group array {
      optional boolean addon_compatibility_check_enabled;
      optional boolean blocklist_enabled;
      optional boolean is_default_browser;
      optional binary default_search_engine (UTF8);
      optional group default_search_engine_data {
        optional binary name (UTF8);
        optional binary load_path (UTF8);
        optional binary submission_url (UTF8);
      }
      optional binary search_cohort (UTF8);
      optional boolean e10s_enabled;
      optional boolean telemetry_enabled;
      optional binary locale (UTF8);
      optional group update {
        optional binary channel (UTF8);
        optional boolean enabled;
        optional boolean auto_download;
      }
      optional group user_prefs (MAP) {
        repeated group map (MAP_KEY_VALUE) {
          required binary key (UTF8);
          required binary value (UTF8);
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(client_id,StringType,false), StructField(normalized_channel,StringType,false), StructField(geo_country,ArrayType(StringType,false),true), StructField(session_length,ArrayType(LongType,false),true), StructField(build,ArrayType(StructType(StructField(application_id,StringType,true), StructField(application_name,StringType,true), StructField(architecture,StringType,true), StructField(architectures_in_binary,StringType,true), StructField(build_id,StringType,true), StructField(version,StringType,true), StructField(vendor,StringType,true), StructField(platform_version,StringType,true), StructField(xpcom_abi,StringType,true), StructField(hotfix_version,StringType,true)),false),true), StructField(settings,ArrayType(StructType(StructField(addon_compatibility_check_enabled,BooleanType,true), StructField(blocklist_enabled,BooleanType,true), StructField(is_default_browser,BooleanType,true), StructField(default_search_engine,StringType,true), StructField(default_search_engine_data,StructType(StructField(name,StringType,true), StructField(load_path,StringType,true), StructField(submission_url,StringType,true)),true), StructField(search_cohort,StringType,true), StructField(e10s_enabled,BooleanType,true), StructField(telemetry_enabled,BooleanType,true), StructField(locale,StringType,true), StructField(update,StructType(StructField(channel,StringType,true), StructField(enabled,BooleanType,true), StructField(auto_download,BooleanType,true)),true), StructField(user_prefs,MapType(StringType,StringType,false),true)),false),true))
       
16/09/07 11:25:11 INFO CatalystReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required binary client_id (UTF8);
  required binary normalized_channel (UTF8);
  optional group geo_country (LIST) {
    repeated binary array (UTF8);
  }
  optional group session_length (LIST) {
    repeated int64 array;
  }
  optional group build (LIST) {
    repeated group array {
      optional binary application_id (UTF8);
      optional binary application_name (UTF8);
      optional binary architecture (UTF8);
      optional binary architectures_in_binary (UTF8);
      optional binary build_id (UTF8);
      optional binary version (UTF8);
      optional binary vendor (UTF8);
      optional binary platform_version (UTF8);
      optional binary xpcom_abi (UTF8);
      optional binary hotfix_version (UTF8);
    }
  }
  optional group settings (LIST) {
    repeated group array {
      optional boolean addon_compatibility_check_enabled;
      optional boolean blocklist_enabled;
      optional boolean is_default_browser;
      optional binary default_search_engine (UTF8);
      optional group default_search_engine_data {
        optional binary name (UTF8);
        optional binary load_path (UTF8);
        optional binary submission_url (UTF8);
      }
      optional binary search_cohort (UTF8);
      optional boolean e10s_enabled;
      optional boolean telemetry_enabled;
      optional binary locale (UTF8);
      optional group update {
        optional binary channel (UTF8);
        optional boolean enabled;
        optional boolean auto_download;
      }
      optional group user_prefs (MAP) {
        repeated group map (MAP_KEY_VALUE) {
          required binary key (UTF8);
          required binary value (UTF8);
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(client_id,StringType,false), StructField(normalized_channel,StringType,false), StructField(geo_country,ArrayType(StringType,false),true), StructField(session_length,ArrayType(LongType,false),true), StructField(build,ArrayType(StructType(StructField(application_id,StringType,true), StructField(application_name,StringType,true), StructField(architecture,StringType,true), StructField(architectures_in_binary,StringType,true), StructField(build_id,StringType,true), StructField(version,StringType,true), StructField(vendor,StringType,true), StructField(platform_version,StringType,true), StructField(xpcom_abi,StringType,true), StructField(hotfix_version,StringType,true)),false),true), StructField(settings,ArrayType(StructType(StructField(addon_compatibility_check_enabled,BooleanType,true), StructField(blocklist_enabled,BooleanType,true), StructField(is_default_browser,BooleanType,true), StructField(default_search_engine,StringType,true), StructField(default_search_engine_data,StructType(StructField(name,StringType,true), StructField(load_path,StringType,true), StructField(submission_url,StringType,true)),true), StructField(search_cohort,StringType,true), StructField(e10s_enabled,BooleanType,true), StructField(telemetry_enabled,BooleanType,true), StructField(locale,StringType,true), StructField(update,StructType(StructField(channel,StringType,true), StructField(enabled,BooleanType,true), StructField(auto_download,BooleanType,true)),true), StructField(user_prefs,MapType(StringType,StringType,false),true)),false),true))
       
16/09/07 11:25:11 INFO CatalystReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required binary client_id (UTF8);
  required binary normalized_channel (UTF8);
  optional group geo_country (LIST) {
    repeated binary array (UTF8);
  }
  optional group session_length (LIST) {
    repeated int64 array;
  }
  optional group build (LIST) {
    repeated group array {
      optional binary application_id (UTF8);
      optional binary application_name (UTF8);
      optional binary architecture (UTF8);
      optional binary architectures_in_binary (UTF8);
      optional binary build_id (UTF8);
      optional binary version (UTF8);
      optional binary vendor (UTF8);
      optional binary platform_version (UTF8);
      optional binary xpcom_abi (UTF8);
      optional binary hotfix_version (UTF8);
    }
  }
  optional group settings (LIST) {
    repeated group array {
      optional boolean addon_compatibility_check_enabled;
      optional boolean blocklist_enabled;
      optional boolean is_default_browser;
      optional binary default_search_engine (UTF8);
      optional group default_search_engine_data {
        optional binary name (UTF8);
        optional binary load_path (UTF8);
        optional binary submission_url (UTF8);
      }
      optional binary search_cohort (UTF8);
      optional boolean e10s_enabled;
      optional boolean telemetry_enabled;
      optional binary locale (UTF8);
      optional group update {
        optional binary channel (UTF8);
        optional boolean enabled;
        optional boolean auto_download;
      }
      optional group user_prefs (MAP) {
        repeated group map (MAP_KEY_VALUE) {
          required binary key (UTF8);
          required binary value (UTF8);
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(client_id,StringType,false), StructField(normalized_channel,StringType,false), StructField(geo_country,ArrayType(StringType,false),true), StructField(session_length,ArrayType(LongType,false),true), StructField(build,ArrayType(StructType(StructField(application_id,StringType,true), StructField(application_name,StringType,true), StructField(architecture,StringType,true), StructField(architectures_in_binary,StringType,true), StructField(build_id,StringType,true), StructField(version,StringType,true), StructField(vendor,StringType,true), StructField(platform_version,StringType,true), StructField(xpcom_abi,StringType,true), StructField(hotfix_version,StringType,true)),false),true), StructField(settings,ArrayType(StructType(StructField(addon_compatibility_check_enabled,BooleanType,true), StructField(blocklist_enabled,BooleanType,true), StructField(is_default_browser,BooleanType,true), StructField(default_search_engine,StringType,true), StructField(default_search_engine_data,StructType(StructField(name,StringType,true), StructField(load_path,StringType,true), StructField(submission_url,StringType,true)),true), StructField(search_cohort,StringType,true), StructField(e10s_enabled,BooleanType,true), StructField(telemetry_enabled,BooleanType,true), StructField(locale,StringType,true), StructField(update,StructType(StructField(channel,StringType,true), StructField(enabled,BooleanType,true), StructField(auto_download,BooleanType,true)),true), StructField(user_prefs,MapType(StringType,StringType,false),true)),false),true))
       
16/09/07 11:25:11 INFO GenerateUnsafeProjection: Code generated in 303.238331 ms
16/09/07 11:25:11 INFO GenerateUnsafeProjection: Code generated in 20.49913 ms
16/09/07 11:25:11 INFO GenerateUnsafeProjection: Code generated in 50.79034 ms
16/09/07 11:25:11 INFO GenerateUnsafeProjection: Code generated in 14.146255 ms
16/09/07 11:25:11 INFO GenerateUnsafeProjection: Code generated in 12.777892 ms
16/09/07 11:25:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 0 records.
16/09/07 11:25:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 6074 records.
16/09/07 11:25:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 4448 records.
16/09/07 11:25:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1910 records.
16/09/07 11:25:11 INFO GenerateSafeProjection: Code generated in 36.852745 ms
16/09/07 11:25:11 INFO InternalParquetRecordReader: at row 0. reading next block
16/09/07 11:25:11 INFO InternalParquetRecordReader: at row 0. reading next block
16/09/07 11:25:11 INFO InternalParquetRecordReader: at row 0. reading next block
16/09/07 11:25:11 INFO Executor: Finished task 2.0 in stage 1.0 (TID 6). 2253 bytes result sent to driver
16/09/07 11:25:11 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 8, localhost, partition 4,PROCESS_LOCAL, 2198 bytes)
16/09/07 11:25:11 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 1525 ms on localhost (1/5)
16/09/07 11:25:11 INFO Executor: Running task 4.0 in stage 1.0 (TID 8)
16/09/07 11:25:11 INFO CodecPool: Got brand-new decompressor [.snappy]
16/09/07 11:25:11 INFO CodecPool: Got brand-new decompressor [.snappy]
16/09/07 11:25:11 INFO CodecPool: Got brand-new decompressor [.snappy]
16/09/07 11:25:11 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 1910
16/09/07 11:25:11 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 4448
16/09/07 11:25:11 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 6074
16/09/07 11:25:11 INFO ParquetRelation$$anonfun$buildInternalScan$1$$anon$1: Input split: ParquetInputSplit{part: file:/home/harterrt/data/l10l_20160725_single_shard.parquet start: 536870912 end: 658792014 length: 121921102 hosts: []}
16/09/07 11:25:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
16/09/07 11:25:12 INFO CatalystReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required binary client_id (UTF8);
  required binary normalized_channel (UTF8);
  optional group geo_country (LIST) {
    repeated binary array (UTF8);
  }
  optional group session_length (LIST) {
    repeated int64 array;
  }
  optional group build (LIST) {
    repeated group array {
      optional binary application_id (UTF8);
      optional binary application_name (UTF8);
      optional binary architecture (UTF8);
      optional binary architectures_in_binary (UTF8);
      optional binary build_id (UTF8);
      optional binary version (UTF8);
      optional binary vendor (UTF8);
      optional binary platform_version (UTF8);
      optional binary xpcom_abi (UTF8);
      optional binary hotfix_version (UTF8);
    }
  }
  optional group settings (LIST) {
    repeated group array {
      optional boolean addon_compatibility_check_enabled;
      optional boolean blocklist_enabled;
      optional boolean is_default_browser;
      optional binary default_search_engine (UTF8);
      optional group default_search_engine_data {
        optional binary name (UTF8);
        optional binary load_path (UTF8);
        optional binary submission_url (UTF8);
      }
      optional binary search_cohort (UTF8);
      optional boolean e10s_enabled;
      optional boolean telemetry_enabled;
      optional binary locale (UTF8);
      optional group update {
        optional binary channel (UTF8);
        optional boolean enabled;
        optional boolean auto_download;
      }
      optional group user_prefs (MAP) {
        repeated group map (MAP_KEY_VALUE) {
          required binary key (UTF8);
          required binary value (UTF8);
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(client_id,StringType,false), StructField(normalized_channel,StringType,false), StructField(geo_country,ArrayType(StringType,false),true), StructField(session_length,ArrayType(LongType,false),true), StructField(build,ArrayType(StructType(StructField(application_id,StringType,true), StructField(application_name,StringType,true), StructField(architecture,StringType,true), StructField(architectures_in_binary,StringType,true), StructField(build_id,StringType,true), StructField(version,StringType,true), StructField(vendor,StringType,true), StructField(platform_version,StringType,true), StructField(xpcom_abi,StringType,true), StructField(hotfix_version,StringType,true)),false),true), StructField(settings,ArrayType(StructType(StructField(addon_compatibility_check_enabled,BooleanType,true), StructField(blocklist_enabled,BooleanType,true), StructField(is_default_browser,BooleanType,true), StructField(default_search_engine,StringType,true), StructField(default_search_engine_data,StructType(StructField(name,StringType,true), StructField(load_path,StringType,true), StructField(submission_url,StringType,true)),true), StructField(search_cohort,StringType,true), StructField(e10s_enabled,BooleanType,true), StructField(telemetry_enabled,BooleanType,true), StructField(locale,StringType,true), StructField(update,StructType(StructField(channel,StringType,true), StructField(enabled,BooleanType,true), StructField(auto_download,BooleanType,true)),true), StructField(user_prefs,MapType(StringType,StringType,false),true)),false),true))
       
16/09/07 11:25:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 0 records.
16/09/07 11:25:12 INFO Executor: Finished task 4.0 in stage 1.0 (TID 8). 2253 bytes result sent to driver
16/09/07 11:25:12 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 8) in 293 ms on localhost (2/5)
16/09/07 11:25:12 INFO GenerateSafeProjection: Code generated in 238.02033 ms
16/09/07 11:25:12 INFO GenerateUnsafeProjection: Code generated in 42.622888 ms
16/09/07 11:25:12 INFO Executor: Finished task 1.0 in stage 1.0 (TID 5). 2253 bytes result sent to driver
16/09/07 11:25:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 2232 ms on localhost (3/5)
16/09/07 11:25:12 INFO Executor: Finished task 3.0 in stage 1.0 (TID 7). 2253 bytes result sent to driver
16/09/07 11:25:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4). 2253 bytes result sent to driver
16/09/07 11:25:12 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 2234 ms on localhost (4/5)
16/09/07 11:25:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 2243 ms on localhost (5/5)
16/09/07 11:25:12 INFO DAGScheduler: ShuffleMapStage 1 (take at CrossSectionalView.scala:169) finished in 2.243 s
16/09/07 11:25:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
16/09/07 11:25:12 INFO DAGScheduler: looking for newly runnable stages
16/09/07 11:25:12 INFO DAGScheduler: running: Set()
16/09/07 11:25:12 INFO DAGScheduler: waiting: Set(ResultStage 2)
16/09/07 11:25:12 INFO DAGScheduler: failed: Set()
16/09/07 11:25:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at take at CrossSectionalView.scala:169), which has no missing parents
16/09/07 11:25:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 228.4 KB, free 644.2 KB)
16/09/07 11:25:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 70.0 KB, free 714.2 KB)
16/09/07 11:25:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:37479 (size: 70.0 KB, free: 1140.2 MB)
16/09/07 11:25:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
16/09/07 11:25:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at take at CrossSectionalView.scala:169)
16/09/07 11:25:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
16/09/07 11:25:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 9, localhost, partition 0,NODE_LOCAL, 1894 bytes)
16/09/07 11:25:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 9)
16/09/07 11:25:12 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks out of 5 blocks
16/09/07 11:25:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
16/09/07 11:25:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 9). 1402 bytes result sent to driver
16/09/07 11:25:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 9) in 84 ms on localhost (1/1)
16/09/07 11:25:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
16/09/07 11:25:12 INFO DAGScheduler: ResultStage 2 (take at CrossSectionalView.scala:169) finished in 0.085 s
16/09/07 11:25:12 INFO DAGScheduler: Job 1 finished: take at CrossSectionalView.scala:169, took 2.702824 s
16/09/07 11:25:12 INFO GenerateSafeProjection: Code generated in 12.899597 ms
================================================================================
[Lcom.mozilla.telemetry.views.CrossSectional;@19709e24
================================================================================
16/09/07 11:25:12 ERROR ContextCleaner: Error in cleaning thread
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:176)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1180)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:173)
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:68)
16/09/07 11:25:12 ERROR Utils: uncaught error in thread SparkListenerBus, stopping SparkContext
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:996)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:317)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:66)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1180)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)
[0m[[32msuccess[0m] [0mTotal time: 29 s, completed Sep 7, 2016 11:25:12 AM[0m
16/09/07 11:25:12 INFO SparkUI: Stopped Spark web UI at http://192.168.1.154:4040
16/09/07 11:25:12 INFO DiskBlockManager: Shutdown hook called
16/09/07 11:25:12 INFO ShutdownHookManager: Shutdown hook called
16/09/07 11:25:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4459256-6d40-4d68-8416-b0ab9a1ee5c9/userFiles-31f09da4-f1f4-4a82-8495-41fd87598aa8
16/09/07 11:25:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-561bd2ef-8239-4ea1-82bb-8694f51e18f2
16/09/07 11:25:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4459256-6d40-4d68-8416-b0ab9a1ee5c9
